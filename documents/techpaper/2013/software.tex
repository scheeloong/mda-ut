\section{Software}

The submarine's software aware of the competition objectives, and performs computer vision to
command the submarine to move via the embedded system.
The computer vision uses the OpenCV library to help identify objects based on shape and color.
The video input consists of two webcams, one facing forward and one facing down.
These cameras are rigidly attached to the AUV, and we need to move the AUV to change the viewpoint.
The software also includes a graphical simulator that we can use to test our computer vision algorithms.

\subsection{Software Organization}
\label{gui}


\begin{figure}
\begin{center}
 \includegraphics[width=2.2in]{fig/vision}
\caption{Vision modules organization in the AUV.}\label{vision}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
 \includegraphics[width=2.7in]{fig/sim}
\caption{Example image from the simulator showing pipe segments, a box
  and a red buoy in turbid water.}\label{sim}
\end{center}
\end{figure}

The software is broken down into 3
components (Figure~\ref{vision}): (i) the mission software that performs the image
processing; (ii) the simulator that recreates synthetic underwater
images based on the vehicle position (example in
Figure~\ref{sim}) and (iii) the graphical interface that is used to
control the mission software. The simulator and the graphical
interface were solely created to test and debug the vision
algorithms when AquaTux is not functioning autonomously. All modules connect through TCP/IP in a
very flexible and extensible fashion. This allows us to seamlessly
replace the simulator by the real hardware sensors and electronics
when we take our system from the lab to the water environment. The
graphical interface allows us to access all the data structures in the
AUV and can act as a passive observer, constantly requesting the
state of the vehicle, or as a controller sending commands to the
vessel.



\vspace{-.1in}
\subsection{Image Processing Software}
The image processing software consists of color filters and edge and
line detection algorithms. The color filters allow us to select
specific information to only process relevant parts of the images. The
line processing algorithms use heuristics to form polygons and
recognize objects such as pipes or boxes, that have a well defined
appearance. Our algorithms are robust enough to recognize objects even
if their view changes due to perspective or the presence of debris in
the water.  The mission software consists of a main loop that acquires
images, processes them and send commands to the other subsystems on
one of the two serial ports of the embedded computer (see
Section~\ref{gui}). The mission is broken down into modular tasks that
can me chained or tested individually. Tasks proceed by first
identifying a target for the vehicle to begin pursuit. If the object
in question is not found, the vehicle rotates on its vertical axis
(yaw) to make the cameras pan to the right and to the left. If nothing
is found, then the vehicle proceeds forward in its initial direction.
Once a target is acquired, the vertical position and lateral positions
of the vessel are controlled to match the desired coordinates.  The
vehicle uses a combination of heading relative to its current
orientation and absolute headings (relative to the magnetic north
pole) to navigate inside the environment. This alleviates the need to
get real-time measurements of the heading as the vehicle is
navigating. A limitation of our vision hardware that we had to address
is to determine when a task is completed if the view of the target is
obscured or occluded due to the AUV's proximity (e.g. it is passing
under the gate). In such situations, for the moment being, we use
timers to ensure that the vehicle reaches its target location.
